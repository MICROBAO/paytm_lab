{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c3dd4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, FloatType, DoubleType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "620ce8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/25 06:27:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"paytm_weather_analysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4451e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the paths with the right ones to make codes running on other machines\n",
    "weather_data_path='/Users/dongzheli/Desktop/paytmteam_weather/data/2019/'\n",
    "station_list_path='/Users/dongzheli/Desktop/paytmteam_weather/stationlist.csv'\n",
    "country_list_path='/Users/dongzheli/Desktop/paytmteam_weather/countrylist.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c6f634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predefine schema for large dataset ingestion\n",
    "weather_schema = StructType([StructField('STN---', IntegerType(), True),\n",
    "                 StructField('WBAN', IntegerType(), True),\n",
    "                 StructField('YEARMODA', IntegerType(), True),\n",
    "                 StructField('TEMP', DoubleType(), True),                  \n",
    "                 StructField('DEWP', DoubleType(), True),      \n",
    "                 StructField('SLP', DoubleType(), True),\n",
    "                 StructField('STP', DoubleType(), True),\n",
    "                 StructField('VISIB', DoubleType(), True),\n",
    "                 StructField('WDSP', DoubleType(), True),\n",
    "                 StructField('MXSPD', DoubleType(), True),       \n",
    "                 StructField('GUST', DoubleType(), True),       \n",
    "                 StructField('MAX', DoubleType(), True),       \n",
    "                 StructField('MIN', DoubleType(), True),                 \n",
    "                 StructField('PRCP', DoubleType(), True),       \n",
    "                 StructField('SNDP', DoubleType(), True),       \n",
    "                 StructField('FRSHTT', StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f81ce32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+----+----+------+------+-----+----+-----+-----+----+----+----+-----+------+\n",
      "|STN---|WBAN |YEARMODA|TEMP|DEWP|SLP   |STP   |VISIB|WDSP|MXSPD|GUST |MAX |MIN |PRCP|SNDP |FRSHTT|\n",
      "+------+-----+--------+----+----+------+------+-----+----+-----+-----+----+----+----+-----+------+\n",
      "|10260 |99999|20190101|26.1|21.2|1001.9|987.5 |20.6 |9.0 |15.9 |29.7 |29.8|NULL|NULL|18.5 |001000|\n",
      "|10260 |99999|20190102|24.9|22.1|1020.1|1005.5|5.4  |5.6 |13.6 |22.1 |NULL|20.7|NULL|22.8 |001000|\n",
      "|10260 |99999|20190103|31.7|29.1|1008.9|994.7 |13.6 |11.6|21.4 |49.5 |NULL|NULL|NULL|999.9|011000|\n",
      "|10260 |99999|20190104|32.9|30.3|1011.4|997.1 |15.8 |4.9 |7.8  |10.9 |36.1|31.8|NULL|999.9|001000|\n",
      "|10260 |99999|20190105|35.5|33.0|1015.7|1001.4|12.0 |10.4|13.6 |21.0 |NULL|32.7|NULL|23.6 |010000|\n",
      "|10260 |99999|20190106|38.5|34.1|1008.2|994.2 |12.8 |10.0|17.5 |28.9 |41.4|NULL|NULL|23.2 |010000|\n",
      "|10260 |99999|20190107|32.1|29.8|996.8 |982.7 |6.9  |11.3|15.5 |28.6 |NULL|30.4|NULL|999.9|001000|\n",
      "|10260 |99999|20190108|31.6|28.0|997.4 |983.3 |22.9 |5.9 |11.7 |19.0 |34.3|NULL|NULL|0.4  |011000|\n",
      "|10260 |99999|20190109|29.9|27.7|1011.6|997.3 |29.8 |7.6 |15.2 |26.6 |32.4|26.1|NULL|23.6 |001000|\n",
      "|10260 |99999|20190110|33.1|30.6|979.1 |965.3 |5.3  |17.8|24.9 |41.8 |41.4|NULL|NULL|999.9|011000|\n",
      "|10260 |99999|20190111|31.2|29.0|975.0 |961.1 |5.6  |11.6|17.5 |38.9 |NULL|NULL|NULL|0.4  |011100|\n",
      "|10260 |99999|20190112|28.3|26.1|988.2 |974.1 |8.2  |8.1 |13.6 |38.5 |NULL|NULL|NULL|999.9|001000|\n",
      "|10260 |99999|20190113|22.7|20.9|977.1 |963.0 |26.6 |4.1 |7.8  |15.2 |27.7|NULL|NULL|0.4  |001000|\n",
      "|10260 |99999|20190114|20.0|18.3|984.3 |970.0 |43.1 |3.6 |9.7  |10.7 |NULL|15.4|NULL|38.6 |000000|\n",
      "|10260 |99999|20190115|25.9|23.2|991.3 |977.1 |16.0 |7.4 |13.8 |20.8 |27.3|19.2|NULL|999.9|001000|\n",
      "|10260 |99999|20190116|24.8|21.8|992.5 |978.2 |33.4 |2.7 |5.8  |999.9|26.1|23.5|NULL|35.4 |001000|\n",
      "|10260 |99999|20190117|21.4|19.0|989.8 |975.5 |10.4 |6.1 |8.9  |13.4 |NULL|NULL|NULL|35.0 |001000|\n",
      "|10260 |99999|20190118|21.0|19.1|994.4 |980.0 |13.8 |5.6 |7.8  |12.8 |22.3|19.2|NULL|35.0 |001000|\n",
      "|10260 |99999|20190119|20.2|18.5|1000.8|986.3 |33.9 |3.4 |7.8  |10.3 |NULL|NULL|NULL|35.8 |001000|\n",
      "|10260 |99999|20190120|21.7|18.5|1009.0|994.4 |32.1 |9.5 |14.6 |20.4 |NULL|18.7|NULL|999.9|001000|\n",
      "+------+-----+--------+----+----+------+------+-----+----+-----+-----+----+----+----+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_df = spark.read.csv(weather_data_path, header=True, schema=weather_schema)\n",
    "weather_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34bdef92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "|STN_NO|COUNTRY_ABBR|\n",
      "+------+------------+\n",
      "|12240 |NO          |\n",
      "|20690 |SW          |\n",
      "|20870 |SW          |\n",
      "|21190 |SW          |\n",
      "|32690 |UK          |\n",
      "|33450 |UK          |\n",
      "|39290 |UK          |\n",
      "|39790 |EI          |\n",
      "|40480 |IC          |\n",
      "|41300 |IC          |\n",
      "|60100 |FO          |\n",
      "|61443 |DA          |\n",
      "|63401 |NL          |\n",
      "|71910 |FR          |\n",
      "|92640 |GM          |\n",
      "|123766|PL          |\n",
      "|125990|PL          |\n",
      "|129700|HU          |\n",
      "|132240|HR          |\n",
      "|156500|BU          |\n",
      "+------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "station_schema = StructType(\n",
    "                    [StructField('STN_NO', IntegerType(), True),\n",
    "                     StructField('COUNTRY_ABBR', StringType(), True)]\n",
    "                    )\n",
    "station_df = spark.read.csv(station_list_path, header=True, schema=station_schema)\n",
    "station_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dcc6fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------------------+\n",
      "|COUNTRY_ABBR|COUNTRY_FULL               |\n",
      "+------------+---------------------------+\n",
      "|AA          |ARUBA                      |\n",
      "|AC          |ANTIGUA AND BARBUDA        |\n",
      "|AF          |AFGHANISTAN                |\n",
      "|AG          |ALGERIA                    |\n",
      "|AI          |ASCENSION ISLAND           |\n",
      "|AJ          |AZERBAIJAN                 |\n",
      "|AL          |ALBANIA                    |\n",
      "|AM          |ARMENIA                    |\n",
      "|AN          |ANDORRA                    |\n",
      "|AO          |ANGOLA                     |\n",
      "|AQ          |AMERICAN SAMOA             |\n",
      "|AR          |ARGENTINA                  |\n",
      "|AS          |AUSTRALIA                  |\n",
      "|AT          |ASHMORE AND CARTIER ISLANDS|\n",
      "|AU          |AUSTRIA                    |\n",
      "|AV          |ANGUILLA                   |\n",
      "|AX          |ANTIGUA                    |\n",
      "|AY          |ANTARCTICA                 |\n",
      "|AZ          |AZORES                     |\n",
      "|BA          |BAHRAIN                    |\n",
      "+------------+---------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_schema = StructType(\n",
    "                    [StructField('COUNTRY_ABBR', StringType(), True),\n",
    "                     StructField('COUNTRY_FULL', StringType(), True)]\n",
    "                    )\n",
    "country_df = spark.read.csv(country_list_path, header=True, schema=country_schema)\n",
    "country_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "776612b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|COUNTRY_ABBR|COUNTRY_FULL|\n",
      "+------------+------------+\n",
      "|           0|           0|\n",
      "+------------+------------+\n",
      "\n",
      "+------------+-----+\n",
      "|COUNTRY_FULL|count|\n",
      "+------------+-----+\n",
      "|       KOREA|    2|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cleaning up and validation\n",
    "\n",
    "from pyspark.sql.functions import col,isnan, when, count, to_date\n",
    "country_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in country_df.columns]\n",
    "   ).show()\n",
    "#No nulls in country_df\n",
    "country_df.groupBy(\"COUNTRY_FULL\").count().where(\"count > 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0fc5f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "|STN_NO|COUNTRY_ABBR|\n",
      "+------+------------+\n",
      "|    40|           0|\n",
      "+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "station_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in station_df.columns]\n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6477b50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25266"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station_df_cleaned = station_df.na.drop(subset=[\"STN_NO\"])\n",
    "station_df_cleaned.count()\n",
    "# 25266 removed 40 nulls where STN_NO is NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fd22740",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+--------+----+----+---+---+-----+----+-----+----+-------+-------+-------+----+------+\n",
      "|STN---|WBAN|YEARMODA|TEMP|DEWP|SLP|STP|VISIB|WDSP|MXSPD|GUST|    MAX|    MIN|   PRCP|SNDP|FRSHTT|\n",
      "+------+----+--------+----+----+---+---+-----+----+-----+----+-------+-------+-------+----+------+\n",
      "|     0|   0|       0|   0|   0|  0|  0|    0|   0|    0|   0|1814196|1723358|3397649|   0|     0|\n",
      "+------+----+--------+----+----+---+---+-----+----+-----+----+-------+-------+-------+----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 15:==========================================================(5 + 0) / 5]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Clean up weather_df\n",
    "weather_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in weather_df.columns]\n",
    "   ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "172027d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:==================================>                       (3 + 2) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------+-------+---------+--------+---------+--------+-------+-------+--------+--------+\n",
      "|TEMP_MAX|DEWP_MAX|SLP_MAX|STP_MAX|VISIB_MAX|WDSP_MAX|MXSPD_MAX|GUST_MAX|MAX_MAX|MIN_MAX|PRCP_MAX|SNDP_MAX|\n",
      "+--------+--------+-------+-------+---------+--------+---------+--------+-------+-------+--------+--------+\n",
      "|   110.0|  9999.9| 9999.9| 9999.9|    999.9|   999.9|    999.9|   999.9| 9999.9| 9999.9|   99.99|   999.9|\n",
      "+--------+--------+-------+-------+---------+--------+---------+--------+-------+-------+--------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 18:==============================================>           (4 + 1) / 5]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "weather_df.select(\n",
    "                    max(weather_df.TEMP).alias(\"TEMP_MAX\"), \n",
    "                    max(weather_df.DEWP).alias(\"DEWP_MAX\"),\n",
    "                    max(weather_df.SLP).alias(\"SLP_MAX\"),\n",
    "                    max(weather_df.STP).alias(\"STP_MAX\"),\n",
    "                    max(weather_df.VISIB).alias(\"VISIB_MAX\"),\n",
    "                    max(weather_df.WDSP).alias(\"WDSP_MAX\"),\n",
    "                    max(weather_df.MXSPD).alias(\"MXSPD_MAX\"),\n",
    "                    max(weather_df.GUST).alias(\"GUST_MAX\"),\n",
    "                    max(weather_df.MAX).alias(\"MAX_MAX\"),\n",
    "                    max(weather_df.MIN).alias(\"MIN_MAX\"),\n",
    "                    max(weather_df.PRCP).alias(\"PRCP_MAX\"),\n",
    "                    max(weather_df.SNDP).alias(\"SNDP_MAX\"),\n",
    "                ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc146c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values with null\n",
    "# Rename STN--- to STN_NO\n",
    "# Convert YEARMODA from string to date\n",
    "weather_df_fillnull = weather_df.withColumnRenamed(\"STN---\", \"STN_NO\") \\\n",
    "            .withColumn(\"YEARMODA\", to_date(weather_df.YEARMODA, 'yyyyMMdd')) \\\n",
    "            .withColumn(\"TEMP\", when(weather_df.TEMP == 9999.9, None).otherwise(weather_df.TEMP)) \\\n",
    "            .withColumn(\"DEWP\", when(weather_df.DEWP == 9999.9, None).otherwise(weather_df.DEWP)) \\\n",
    "            .withColumn(\"SLP\", when(weather_df.SLP == 9999.9, None).otherwise(weather_df.SLP)) \\\n",
    "            .withColumn(\"STP\", when(weather_df.STP == 9999.9, None).otherwise(weather_df.STP)) \\\n",
    "            .withColumn(\"VISIB\", when(weather_df.VISIB == 999.9, None).otherwise(weather_df.VISIB)) \\\n",
    "            .withColumn(\"WDSP\", when(weather_df.WDSP == 999.9, None).otherwise(weather_df.WDSP)) \\\n",
    "            .withColumn(\"MXSPD\", when(weather_df.MXSPD == 999.9, None).otherwise(weather_df.MXSPD)) \\\n",
    "            .withColumn(\"GUST\", when(weather_df.GUST == 999.9, None).otherwise(weather_df.GUST)) \\\n",
    "            .withColumn(\"MAX\", when(weather_df.MAX == 9999.9, None).otherwise(weather_df.MAX)) \\\n",
    "            .withColumn(\"MIN\", when(weather_df.MIN == 9999.9, None).otherwise(weather_df.MIN)) \\\n",
    "            .withColumn(\"PRCP\", when(weather_df.PRCP == 99.99, None).otherwise(weather_df.PRCP)) \\\n",
    "            .withColumn(\"SNDP\", when(weather_df.SNDP == 999.9, None).otherwise(weather_df.SNDP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb30efc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:==================================>                       (3 + 2) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------+-------+---------+--------+---------+--------+-------+-------+--------+--------+\n",
      "|TEMP_MAX|DEWP_MAX|SLP_MAX|STP_MAX|VISIB_MAX|WDSP_MAX|MXSPD_MAX|GUST_MAX|MAX_MAX|MIN_MAX|PRCP_MAX|SNDP_MAX|\n",
      "+--------+--------+-------+-------+---------+--------+---------+--------+-------+-------+--------+--------+\n",
      "|   110.0|    89.5| 1077.4| 1075.2|     91.7|    78.6|     95.0|   116.6|  129.9|  100.0|   12.56|   117.7|\n",
      "+--------+--------+-------+-------+---------+--------+---------+--------+-------+-------+--------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 24:==============================================>           (4 + 1) / 5]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Validate if replace works\n",
    "weather_df_fillnull.select(\n",
    "                    max(weather_df_fillnull.TEMP).alias(\"TEMP_MAX\"), \n",
    "                    max(weather_df_fillnull.DEWP).alias(\"DEWP_MAX\"),\n",
    "                    max(weather_df_fillnull.SLP).alias(\"SLP_MAX\"),\n",
    "                    max(weather_df_fillnull.STP).alias(\"STP_MAX\"),\n",
    "                    max(weather_df_fillnull.VISIB).alias(\"VISIB_MAX\"),\n",
    "                    max(weather_df_fillnull.WDSP).alias(\"WDSP_MAX\"),\n",
    "                    max(weather_df_fillnull.MXSPD).alias(\"MXSPD_MAX\"),\n",
    "                    max(weather_df_fillnull.GUST).alias(\"GUST_MAX\"),\n",
    "                    max(weather_df_fillnull.MAX).alias(\"MAX_MAX\"),\n",
    "                    max(weather_df_fillnull.MIN).alias(\"MIN_MAX\"),\n",
    "                    max(weather_df_fillnull.PRCP).alias(\"PRCP_MAX\"),\n",
    "                    max(weather_df_fillnull.SNDP).alias(\"SNDP_MAX\"),\n",
    "                ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71e6161c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------+-------+---------+--------+---------+--------+-------+-------+--------+--------+\n",
      "|TEMP_MIN|DEWP_MIN|SLP_MIN|STP_MIN|VISIB_MIN|WDSP_MIN|MXSPD_MIN|GUST_MIN|MAX_MIN|MIN_MIN|PRCP_MIN|SNDP_MIN|\n",
      "+--------+--------+-------+-------+---------+--------+---------+--------+-------+-------+--------+--------+\n",
      "|  -114.7|  -119.1|  923.5|  547.5|      0.0|     0.0|      0.2|     9.7|  -95.4| -116.0|     0.0|     0.4|\n",
      "+--------+--------+-------+-------+---------+--------+---------+--------+-------+-------+--------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Validate if there are values that are negatives\n",
    "from pyspark.sql.functions import min\n",
    "weather_df_fillnull.select(\n",
    "                    min(weather_df_fillnull.TEMP).alias(\"TEMP_MIN\"), \n",
    "                    min(weather_df_fillnull.DEWP).alias(\"DEWP_MIN\"),\n",
    "                    min(weather_df_fillnull.SLP).alias(\"SLP_MIN\"),\n",
    "                    min(weather_df_fillnull.STP).alias(\"STP_MIN\"),\n",
    "                    min(weather_df_fillnull.VISIB).alias(\"VISIB_MIN\"),\n",
    "                    min(weather_df_fillnull.WDSP).alias(\"WDSP_MIN\"),\n",
    "                    min(weather_df_fillnull.MXSPD).alias(\"MXSPD_MIN\"),\n",
    "                    min(weather_df_fillnull.GUST).alias(\"GUST_MIN\"),\n",
    "                    min(weather_df_fillnull.MAX).alias(\"MAX_MIN\"),\n",
    "                    min(weather_df_fillnull.MIN).alias(\"MIN_MIN\"),\n",
    "                    min(weather_df_fillnull.PRCP).alias(\"PRCP_MIN\"),\n",
    "                    min(weather_df_fillnull.SNDP).alias(\"SNDP_MIN\"),\n",
    "                ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24daa2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+----+----+------+------+-----+----+-----+----+----+----+----+----+------+\n",
      "|STN_NO| WBAN|  YEARMODA|TEMP|DEWP|   SLP|   STP|VISIB|WDSP|MXSPD|GUST| MAX| MIN|PRCP|SNDP|FRSHTT|\n",
      "+------+-----+----------+----+----+------+------+-----+----+-----+----+----+----+----+----+------+\n",
      "| 10260|99999|2019-01-01|26.1|21.2|1001.9| 987.5| 20.6| 9.0| 15.9|29.7|29.8|NULL|NULL|18.5|001000|\n",
      "| 10260|99999|2019-01-02|24.9|22.1|1020.1|1005.5|  5.4| 5.6| 13.6|22.1|NULL|20.7|NULL|22.8|001000|\n",
      "| 10260|99999|2019-01-03|31.7|29.1|1008.9| 994.7| 13.6|11.6| 21.4|49.5|NULL|NULL|NULL|NULL|011000|\n",
      "| 10260|99999|2019-01-04|32.9|30.3|1011.4| 997.1| 15.8| 4.9|  7.8|10.9|36.1|31.8|NULL|NULL|001000|\n",
      "| 10260|99999|2019-01-05|35.5|33.0|1015.7|1001.4| 12.0|10.4| 13.6|21.0|NULL|32.7|NULL|23.6|010000|\n",
      "| 10260|99999|2019-01-06|38.5|34.1|1008.2| 994.2| 12.8|10.0| 17.5|28.9|41.4|NULL|NULL|23.2|010000|\n",
      "| 10260|99999|2019-01-07|32.1|29.8| 996.8| 982.7|  6.9|11.3| 15.5|28.6|NULL|30.4|NULL|NULL|001000|\n",
      "| 10260|99999|2019-01-08|31.6|28.0| 997.4| 983.3| 22.9| 5.9| 11.7|19.0|34.3|NULL|NULL| 0.4|011000|\n",
      "| 10260|99999|2019-01-09|29.9|27.7|1011.6| 997.3| 29.8| 7.6| 15.2|26.6|32.4|26.1|NULL|23.6|001000|\n",
      "| 10260|99999|2019-01-10|33.1|30.6| 979.1| 965.3|  5.3|17.8| 24.9|41.8|41.4|NULL|NULL|NULL|011000|\n",
      "| 10260|99999|2019-01-11|31.2|29.0| 975.0| 961.1|  5.6|11.6| 17.5|38.9|NULL|NULL|NULL| 0.4|011100|\n",
      "| 10260|99999|2019-01-12|28.3|26.1| 988.2| 974.1|  8.2| 8.1| 13.6|38.5|NULL|NULL|NULL|NULL|001000|\n",
      "| 10260|99999|2019-01-13|22.7|20.9| 977.1| 963.0| 26.6| 4.1|  7.8|15.2|27.7|NULL|NULL| 0.4|001000|\n",
      "| 10260|99999|2019-01-14|20.0|18.3| 984.3| 970.0| 43.1| 3.6|  9.7|10.7|NULL|15.4|NULL|38.6|000000|\n",
      "| 10260|99999|2019-01-15|25.9|23.2| 991.3| 977.1| 16.0| 7.4| 13.8|20.8|27.3|19.2|NULL|NULL|001000|\n",
      "| 10260|99999|2019-01-16|24.8|21.8| 992.5| 978.2| 33.4| 2.7|  5.8|NULL|26.1|23.5|NULL|35.4|001000|\n",
      "| 10260|99999|2019-01-17|21.4|19.0| 989.8| 975.5| 10.4| 6.1|  8.9|13.4|NULL|NULL|NULL|35.0|001000|\n",
      "| 10260|99999|2019-01-18|21.0|19.1| 994.4| 980.0| 13.8| 5.6|  7.8|12.8|22.3|19.2|NULL|35.0|001000|\n",
      "| 10260|99999|2019-01-19|20.2|18.5|1000.8| 986.3| 33.9| 3.4|  7.8|10.3|NULL|NULL|NULL|35.8|001000|\n",
      "| 10260|99999|2019-01-20|21.7|18.5|1009.0| 994.4| 32.1| 9.5| 14.6|20.4|NULL|18.7|NULL|NULL|001000|\n",
      "+------+-----+----------+----+----+------+------+-----+----+-----+----+----+----+----+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cleaned up dataframe\n",
    "weather_df_fillnull.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41458391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+--------------+\n",
      "|STN_NO|COUNTRY_ABBR|  COUNTRY_FULL|\n",
      "+------+------------+--------------+\n",
      "| 12240|          NO|        NORWAY|\n",
      "| 20690|          SW|        SWEDEN|\n",
      "| 20870|          SW|        SWEDEN|\n",
      "| 21190|          SW|        SWEDEN|\n",
      "| 32690|          UK|UNITED KINGDOM|\n",
      "| 33450|          UK|UNITED KINGDOM|\n",
      "| 39290|          UK|UNITED KINGDOM|\n",
      "| 39790|          EI|       IRELAND|\n",
      "| 40480|          IC|       ICELAND|\n",
      "| 41300|          IC|       ICELAND|\n",
      "| 60100|          FO| FAROE ISLANDS|\n",
      "| 61443|          DA|       DENMARK|\n",
      "| 63401|          NL|   NETHERLANDS|\n",
      "| 71910|          FR|        FRANCE|\n",
      "| 92640|          GM|       GERMANY|\n",
      "|123766|          PL|        POLAND|\n",
      "|125990|          PL|        POLAND|\n",
      "|129700|          HU|       HUNGARY|\n",
      "|132240|          HR|       CROATIA|\n",
      "|156500|          BU|      BULGARIA|\n",
      "+------+------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Joining station with country to get full country name\n",
    "station_with_country = station_df_cleaned.join(country_df, station_df_cleaned.COUNTRY_ABBR == country_df.COUNTRY_ABBR, \"left\").drop(country_df.COUNTRY_ABBR)\n",
    "station_with_country.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41fc6748",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_with_country = weather_df_fillnull.join(station_with_country, weather_df_fillnull.STN_NO ==  station_with_country.STN_NO, \"left\").drop(station_with_country.STN_NO)\n",
    "weather_with_country.createOrReplaceTempView(\"weather_with_country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8769b2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(ave_mean_tmp=90.06114457831325, COUNTRY_ABBR='DJ', COUNTRY_FULL='DJIBOUTI')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Which country had the hottest average mean temperature over the year?\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    SUM(TEMP)/COUNT(YEARMODA) as ave_mean_tmp,\n",
    "    COUNTRY_ABBR,\n",
    "    COUNTRY_FULL\n",
    "FROM weather_with_country\n",
    "WHERE year(YEARMODA) = 2019 and COUNTRY_FULL IS NOT NULL\n",
    "GROUP BY COUNTRY_ABBR, COUNTRY_FULL\n",
    "ORDER BY ave_mean_tmp DESC\n",
    "\"\"\").head()\n",
    "# DJIBOUTI has the hottest mean temperate over 2019 with 90.06114457831325"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49ec976e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:==============>                                         (4 + 11) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+----------+----------+\n",
      "|  COUNTRY_FULL|CONSECUTIVE_DAYS|START_DATE|  END_DATE|\n",
      "+--------------+----------------+----------+----------+\n",
      "|        CANADA|               2|2019-06-25|2019-06-26|\n",
      "|CAYMAN ISLANDS|               2|2019-10-31|2019-11-01|\n",
      "|         GHANA|               2|2019-09-06|2019-09-07|\n",
      "|         INDIA|               2|2019-09-07|2019-09-08|\n",
      "|         ITALY|               2|2019-01-23|2019-01-24|\n",
      "|         ITALY|               2|2019-10-02|2019-10-03|\n",
      "|         ITALY|               2|2019-11-14|2019-11-15|\n",
      "|         JAPAN|               2|2019-06-10|2019-06-11|\n",
      "|         JAPAN|               2|2019-12-03|2019-12-04|\n",
      "| UNITED STATES|               2|2019-01-12|2019-01-13|\n",
      "| UNITED STATES|               2|2019-06-29|2019-06-30|\n",
      "|       ALGERIA|               1|2019-07-04|2019-07-04|\n",
      "|        ANGOLA|               1|2019-02-06|2019-02-06|\n",
      "|        ANGOLA|               1|2019-04-05|2019-04-05|\n",
      "|      ANGUILLA|               1|2019-06-06|2019-06-06|\n",
      "|     ARGENTINA|               1|2019-05-10|2019-05-10|\n",
      "|         ARUBA|               1|2019-09-23|2019-09-23|\n",
      "|       AUSTRIA|               1|2019-01-20|2019-01-20|\n",
      "|       AUSTRIA|               1|2019-06-16|2019-06-16|\n",
      "|       AUSTRIA|               1|2019-06-19|2019-06-19|\n",
      "+--------------+----------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#2. Which country had the most consecutive days of tornadoes/funnel cloud formations?\n",
    "spark.sql(\"\"\"\n",
    "WITH IS_TORNADO_OR_FUNNEL_CTE as ( \n",
    "    SELECT  \n",
    "        COUNTRY_FULL,\n",
    "        YEARMODA,\n",
    "        SUM(CAST(RIGHT(FRSHTT, 1) AS int)) AS IS_TORNADO_OR_FUNNEKL\n",
    "    FROM weather_with_country\n",
    "    WHERE YEAR(YEARMODA) = 2019 AND COUNTRY_FULL IS NOT NULL\n",
    "    GROUP BY COUNTRY_FULL, YEARMODA\n",
    "),\n",
    "ROW_NUMBER_CTE AS \n",
    "(\n",
    "    SELECT \n",
    "        COUNTRY_FULL,\n",
    "        YEARMODA,\n",
    "        IS_TORNADO_OR_FUNNEKL,\n",
    "        ROW_NUMBER() OVER(PARTITION BY COUNTRY_FULL ORDER BY YEARMODA) as row_number\n",
    "    FROM IS_TORNADO_OR_FUNNEL_CTE\n",
    "    WHERE IS_TORNADO_OR_FUNNEKL > 0\n",
    ")\n",
    "SELECT \n",
    "    COUNTRY_FULL,\n",
    "    COUNT(*) as CONSECUTIVE_DAYS,\n",
    "    MIN(YEARMODA) AS START_DATE,\n",
    "    MAX(YEARMODA) AS END_DATE\n",
    "FROM ROW_NUMBER_CTE\n",
    "GROUP BY COUNTRY_FULL,  DATE_ADD(day, -row_number, YEARMODA)\n",
    "ORDER BY CONSECUTIVE_DAYS DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32649c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/25 07:53:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(COUNTRY_FULL='FALKLAND ISLANDS (ISLAS MALVINAS)', SECOND_HIGHEST=17.84236111111111)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Which country had the second highest average mean wind speed over the year?\n",
    "spark.sql(\"\"\"\n",
    "WITH AVE_MEAN_WDSP AS (\n",
    "SELECT\n",
    "    SUM(WDSP)/COUNT(YEARMODA) as AVE_MEAN_WDSP,\n",
    "    COUNTRY_FULL\n",
    "FROM weather_with_country\n",
    "WHERE year(YEARMODA) = 2019 and COUNTRY_FULL IS NOT NULL\n",
    "GROUP BY COUNTRY_ABBR, COUNTRY_FULL\n",
    ")\n",
    "SELECT \n",
    "    COUNTRY_FULL,\n",
    "    MAX(AVE_MEAN_WDSP) AS SECOND_HIGHEST\n",
    "FROM AVE_MEAN_WDSP \n",
    "WHERE AVE_MEAN_WDSP < (SELECT MAX(AVE_MEAN_WDSP) FROM AVE_MEAN_WDSP)\n",
    "GROUP BY COUNTRY_FULL\n",
    "ORDER BY SECOND_HIGHEST DESC\n",
    "\"\"\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7906fcca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
